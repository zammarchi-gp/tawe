#Step 1 ####
#load output from LDA (or other topic modelling technique) and word embedding. 
#Create a ds object with all the topics, similar to the example given below.

ds = structure(list(journal = c("guardian", "nytimes", "washingtonpost"), topic = c("Topic 1", "Topic 1", "Topic 1"), 
                     P1 = c("0.018", "0.017", "0.037"), W1 = c("view", "study", "biden"), P2 = c("0.014", "0.014", "0.022"), W2 = c("editorial", "water", "analysis"), P3 = c("0.013", "0.011", "0.022"), W3 = c("crisis", "found", "trump"), P4 = c("0.01", "0.011", "0.019"), W4 = c("people", "levels", "administration"), P5 = c("0.009", "0.009", "0.018"), W5 = c("delhi", "catastrophic", "energy"), P6 = c("0.009", "0.009", "0.015"), W6 = c("video", "glaciers", "president"), P7 = c("0.009", "0.009", "0.012"), W7 = c("paris", "home", "fight"), P8 = c("0.008", "0.009", "0.011"), W8 = c("fight", "worse", "action"), P9 = c("0.008", "0.009", "0.011"), W9 = c("don", "coal", "finds"), P10 = c("0.008", "0.008", "0.01"), W10 = c("key", "experts", "study"), 
                     P11 = c("0.007", "0.008", "0.008"), W11 = c("beijing", "infrastructure", "plans"), P12 = c("0.007", "0.008", "0.008"), W12 = c("worlds", "hundreds", "renewable"), P13 = c("0.007", "0.008", "0.008"), W13 = c("action", "lead", "bill"), P14 = c("0.007", "0.008", "0.008"), W14 = c("house", "india", "crisis"), P15 = c("0.006", "0.008", "0.007"), W15 = c("plan", "coming", "emissions"), P16 = c("0.006", "0.007", "0.007"), W16 = c("pacific", "live", "wildfires"), P17 = c("0.006", "0.007", "0.007"), W17 = c("policy", "gas", "gas"), P18 = c("0.006", "0.007", "0.007"), W18 = c("science", "energy", "environmental"), P19 = c("0.006", "0.007", "0.007"), W19 = c("whats", "things", "major"), P20 = c("0.006", "0.007", "0.006"), W20 = c("giant", "high", "government"), 
                     P21 = c("0.006", "0.007", "0.006"), W21 = c("problem", "emissions", "oil"), P22 = c("0.006", "0.007", "0.006"), W22 = c("fears", "years", "combat"), P23 = c("0.006", "0.007", "0.006"), W23 = c("president", "oil", "forests"), P24 = c("0.006", "0.007", "0.006"), W24 = c("lost", "environmental", "due"), P25 = c("0.005", "0.006", "0.006"), W25 = c("white", "impacts", "budget"), P26 = c("0.005", "0.006", "0.006"), W26 = c("residents", "racial", "food"), P27 = c("0.005", "0.006", "0.005"), W27 = c("young", "increase", "effort"), P28 = c("0.005", "0.006", "0.005"), W28 = c("put", "parts", "report"), P29 = c("0.005", "0.006", "0.005"), W29 = c("live", "future", "epa"), P30 = c("0.005", "0.006", "0.005"), W30 = c("officials", "expected", "bears"), 
                     P31 = c("0.005", "0.006", "0.005"), W31 = c("calls", "make", "research"), P32 = c("0.005", "0.006", "0.005"), W32 = c("cities", "britain", "ice"), P33 = c("0.005", "0.006", "0.005"), W33 = c("campaign", "scientists", "hurricane"), P34 = c("0.005", "0.005", "0.005"), W34 = c("rising", "weather", "joins"), P35 = c("0.005", "0.005", "0.005"), W35 = c("john", "monday", "set"), P36 = c("0.005", "0.005", "0.005"), W36 = c("time", "fueled", "amid"), P37 = c("0.005", "0.005", "0.005"), W37 = c("cut", "event", "time"), P38 = c("0.005", "0.005", "0.005"), W38 = c("smog", "bad", "shows"), P39 = c("0.005", "0.005", "0.005"), W39 = c("carbon", "isn", "federal"), P40 = c("0.005", "0.005", "0.005"), W40 = c("effects", "decades", "nation"), 
                     P41 = c("0.005", "0.005", "0.005"), W41 = c("group", "plants", "including"), P42 = c("0.004", "0.005", "0.005"), W42 = c("agreement", "agency", "policy"), P43 = c("0.004", "0.004", "0.004"), W43 = c("drive", "state", "fire"), P44 = c("0.004", "0.004", "0.004"), W44 = c("answer", "death", "real"), P45 = c("0.004", "0.004", "0.004"), W45 = c("waste", "toll", "issues"), P46 = c("0.004", "0.004", "0.004"), W46 = c("report", "caused", "australia"), P47 = c("0.004", "0.004", "0.004"), W47 = c("water", "action", "plan"), P48 = c("0.004", "0.004", "0.004"), W48 = c("accused", "single", "republicans"), P49 = c("0.004", "0.004", "0.004"), W49 = c("killed", "significantly", "california"), P50 = c("0.004", "0.004", "0.004"), W50 = c("home", "methane", "conference"), 
                     P51 = c("0.004", "0.004", "0.004"), W51 = c("plans", "move", "dangerous"), P52 = c("0.004", "0.004", "0.004"), W52 = c("talks", "news", "leading"), P53 = c("0.004", "0.004", "0.004"), W53 = c("abraham", "inequality", "levels"), P54 = c("0.004", "0.004", "0.004"), W54 = c("combat", "city", "earth"), P55 = c("0.004", "0.004", "0.004"), W55 = c("takes", "taking", "polar"), P56 = c("0.004", "0.004", "0.004"), W56 = c("series", "cities", "chief"), P57 = c("0.004", "0.004", "0.004"), W57 = c("fires", "role", "intensifying"), P58 = c("0.004", "0.004", "0.004"), W58 = c("unveils", "source", "murdoch"), P59 = c("0.004", "0.004", "0.004"), W59 = c("decade", "questions", "released"), P60 = c("0.004", "0.004", "0.004"), W60 = c("city", "class", "act"), 
                     P61 = c("0.004", "0.004", "0.004"), W61 = c("chinas", "pledge", "helped"), P62 = c("0.004", "0.004", "0.004"), W62 = c("big", "students", "show"), P63 = c("0.004", "0.004", "0.003"), W63 = c("oil", "link", "growing"), P64 = c("0.004", "0.004", "0.003"), W64 = c("zealand", "growing", "panel"), P65 = c("0.004", "0.004", "0.003"), W65 = c("traffic", "planet", "al"), P66 = c("0.004", "0.003", "0.003"), W66 = c("ahead", "greenhouse", "gore"), P67 = c("0.004", "0.003", "0.003"), W67 = c("fish", "public", "state"), P68 = c("0.004", "0.003", "0.003"), W68 = c("tories", "limit", "debate"), P69 = c("0.004", "0.003", "0.003"), W69 = c("future", "follow", "part"), P70 = c("0.004", "0.003", "0.003"), W70 = c("speech", "housing", "democrats"), 
                     P71 = c("0.004", "0.003", "0.003"), W71 = c("cruise", "helped", "greenhouse"), P72 = c("0.004", "0.003", "0.003"), W72 = c("laws", "crises", "day"), P73 = c("0.004", "0.003", "0.003"), W73 = c("firms", "answered", "works"), P74 = c("0.004", "0.003", "0.003"), W74 = c("growing", "chemicals", "wildfire"), P75 = c("0.003", "0.003", "0.003"), W75 = c("spikes", "toxic", "bring"), P76 = c("0.003", "0.003", "0.003"), W76 = c("budget", "education", "africa"), P77 = c("0.003", "0.003", "0.003"), W77 = c("china", "sweeping", "consequences"), P78 = c("0.003", "0.003", "0.003"), W78 = c("charge", "mexico", "international"), P79 = c("0.003", "0.003", "0.003"), W79 = c("clean", "fast", "worsen"), P80 = c("0.003", "0.003", "0.003"), W80 = c("thousands", "paris", "tax"), 
                     P81 = c("0.003", "0.003", "0.003"), W81 = c("james", "planning", "good"), P82 = c("0.003", "0.003", "0.003"), W82 = c("urgent", "renewable", "extreme"), P83 = c("0.003", "0.003", "0.003"), W83 = c("food", "crisis", "risk"), P84 = c("0.003", "0.003", "0.003"), W84 = c("government", "researchers", "coal"), P85 = c("0.003", "0.003", "0.003"), W85 = c("roger", "weakened", "plastic"), P86 = c("0.003", "0.003", "0.003"), W86 = c("fossil", "stronger", "sen"), P87 = c("0.003", "0.003", "0.003"), W87 = c("tory", "gathered", "urgency"), P88 = c("0.003", "0.003", "0.003"), W88 = c("deaths", "slowing", "agenda"), P89 = c("0.003", "0.003", "0.003"), W89 = c("north", "protesters", "sign"), P90 = c("0.003", "0.003", "0.003"), W90 = c("failure", "normal", "survive"), 
                     P91 = c("0.003", "0.003", "0.003"), W91 = c("secretary", "wrong", "solve"), P92 = c("0.003", "0.003", "0.003"), W92 = c("flooding", "brink", "fueling"), P93 = c("0.003", "0.003", "0.003"), W93 = c("coal", "airlines", "spending"), P94 = c("0.003", "0.003", "0.003"), W94 = c("turning", "surge", "missing"), P95 = c("0.003", "0.003", "0.003"), W95 = c("exposure", "glacier", "waste"), P96 = c("0.003", "0.003", "0.003"), W96 = c("scientists", "system", "struggling"), P97 = c("0.003", "0.003", "0.003"), W97 = c("podcast", "industry", "coastal"), P98 = c("0.003", "0.003", "0.003"), W98 = c("clothes", "lost", "stop"), P99 = c("0.003", "0.003", "0.003"), W99 = c("ground", "hurricane", "years"), P100 = c("0.003", "0.003", "0.003"), W100 = c("denier", "species", "exxonmobil")), 
                row.names = c(1L, 29L, 39L), class = "data.frame")


#Step 2 ####
#download and read the "enwiki_20180420_100d.txt" file into the object "wikipedia"
wikipedia = scan(file = "enwiki_20180420_100d.txt", what="", sep="\n")

#source: https://gist.github.com/tjvananne/8b0e7df7dcad414e8e6d5bf3947439a9
proc_pretrained_vec <- function(p_vec) {
  vals <- vector(mode = "list", length(p_vec))
  names <- character(length(p_vec))
  
  for(i in 1:length(p_vec)) {
    if(i %% 1000 == 0) {print(i)}
    this_vec <- p_vec[i]
    this_vec_unlisted <- unlist(strsplit(this_vec, " "))
    this_vec_values <- as.numeric(this_vec_unlisted[-1])
    this_vec_name <- this_vec_unlisted[1]
    
    vals[[i]] <- this_vec_values
    names[[i]] <- this_vec_name
  }
  
  glove <- data.frame(vals)
  names(glove) <- names
  
  return(glove)
}

enwiki.100 <- proc_pretrained_vec(wikipedia)
enwiki.100_full.names = names(enwiki.100)

install.packages("remotes")
remotes::install_github("milescsmith/WGCNA") #to use transposeBigData
enwiki.100_transposed = transposeBigData(enwiki.100)
rownames(enwiki.100_transposed) = enwiki.100_full.names
enwiki.100_transposed = enwiki.100_transposed[-1,]


#Step 3 ####
#converting words into vectors

library(lsa)
#set the following parameters based on the number of topics and words
n = 3 
n_words = 100

topic_title = vector()
for (i in 1:n) {
  words = unlist(unname(ds[i, 3:202]))[c(F,T)][1:n_words]
  #the list of words can also be obtained by selecting a specific journal and a topic
  #words = unique(unlist(unname(c(ds[ds$journal == "guardian" & ds$topic == "Topic 1", paste("W", 1:n_words,sep = "")])))) 
  weights = as.numeric(unlist(unname(ds[i, 3:202]))[c(T,F)])[1:n_words]
  enwiki.100_bow = enwiki.100_transposed[rownames(enwiki.100_transposed) %in% words,]
  
  for(k in 1:nrow(enwiki.100_bow)) {
    if(words[k] %in% rownames(enwiki.100_bow)){
      enwiki.100_bow[words[k],] = enwiki.100_bow[words[k],] * weights[k] 
    }
  }
  
  medoid = apply(enwiki.100_bow, 2, median)
  
  similarity = cosine(medoid, t(enwiki.100_bow)) 
  names(sort(similarity, decreasing=T)[1]) 
  topic_title[i] = names(sort(similarity, decreasing=T)[1]) 
}
topic_title


#Step 4 ####
#computing similarities

#library(lsa)
titles_GPT = c("coverage", "catastrophic", "analysis")
titlesGPT_restricted = c("crisis", "levels", "analysis")
titles_hiprob = c("view", "study", "biden")
titles_tawe = topic_title

#creating a matrix with all the comparisons
all_titles = c(titles_hiprob, titles_tawe, titles_GPT, titlesGPT_restricted) 
titles_WE = enwiki.100_transposed[rownames(enwiki.100_transposed) %in% all_titles,] 
matrix_similarity = cosine(t(titles_WE))

#creation of a summary matrix of comparisons
comparisons = as.data.frame(matrix(NA, nrow = length(titles_tawe), ncol = 4))
for(i in 1:nrow(comparisons)) {
  simil_tawe = simil_hiprob = simil_tawe_res = simil_hiprob_res = 0 
  pos1 = which(titles_tawe[i] == rownames(matrix_similarity))
  pos2 = which(titles_hiprob[i] == rownames(matrix_similarity))
  pos3 = which(titles_GPT[i] == rownames(matrix_similarity))
  pos4 = which(titlesGPT_restricted[i] == rownames(matrix_similarity))
  
  simil_tawe  = matrix_similarity[pos1, pos3] #TAWE vs chatGPT
  simil_hiprob = matrix_similarity[pos2, pos3] #HiProb vs chatGPT
  simil_tawe_res  = matrix_similarity[pos1, pos4] #TAWE vs chatGPT (restricted)
  simil_hiprob_res = matrix_similarity[pos2, pos4] #HiProb vs chatGPT (restricted)
  
  comparisons[i, 1] = simil_tawe
  comparisons[i, 2] = simil_hiprob
  comparisons[i, 3] = simil_tawe_res
  comparisons[i, 4] = simil_hiprob_res
  
}
names(comparisons) = c("tawe_vs_chatGPT", "hiprob_vs_chatGPT", "tawe_vs_chatGPT_restricted", "hiprob_vs_chatGPT_restricted")
comparisons$tawe = titles_tawe
comparisons$hiprob = titles_hiprob
comparisons$chatGPT = titles_GPT
comparisons$chatGPT_restricted = titlesGPT_restricted

comparisons = comparisons[, c(5, 6, 7, 1, 2, 8, 3, 4)] #rearranged for clarity

